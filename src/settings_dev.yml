model_id: qwen/qwen3-0.6b
max_model_len: 200
lora_rank: 8
lora_alpha: 8
tensor_parallel_size: 1

datagen:answers_per_question: 2
datagen:gpu_memory_utilization: 0.8

sft:num_epochs: 1
sft:save_interval: 10
sft:packing: False
sft:per_device_train_batch_size: 1

eval:num_epochs: 1
eval:max_connections: 2
eval:gpu_memory_utilization: 0.8
